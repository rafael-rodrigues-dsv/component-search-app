"""
Camada de Aplica√ß√£o - Servi√ßo principal do rob√¥ coletor
"""
import random
import time
import zipfile
from typing import List

import requests
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from config.settings import *
from ..domain.email_processor import (
    SearchTerm, EmailCollectorInterface,
    WorkingHoursService, SearchTermBuilder, EmailValidationService
)
from ..infrastructure.repositories.data_persistence import JsonRepository, ExcelRepository
from ..infrastructure.scrapers.duckduckgo_scraper import DuckDuckGoScraper
from ..infrastructure.scrapers.google_scraper import GoogleScraper
from ..infrastructure.web_driver import WebDriverManager


class EmailCollectorService(EmailCollectorInterface):
    """Servi√ßo principal do PythonSearchApp coletor de e-mails"""
    
    # Constantes de configura√ß√£o
    
    def __init__(self, ignore_working_hours=False):
        self.driver_manager = WebDriverManager()
        
        # Escolhe motor de busca
        self.search_engine = self._get_search_engine_option()
        
        # Verifica se deve reiniciar do zero
        if self._get_restart_option():
            self._clear_all_data()
        
        # Escolhe scraper baseado na sele√ß√£o do usu√°rio
        if self.search_engine == "GOOGLE":
            print("[INFO] Usando motor de busca: Google Chrome")
            self.scraper = GoogleScraper(None)  # Driver ser√° definido depois
        else:
            print("[INFO] Usando motor de busca: DuckDuckGo")
            self.scraper = DuckDuckGoScraper(self.driver_manager)
        
        self.json_repo = JsonRepository(VISITED_JSON, SEEN_EMAILS_JSON)
        self.excel_repo = ExcelRepository(OUTPUT_XLSX)
        self.working_hours = WorkingHoursService(START_HOUR, END_HOUR)
        self.ignore_working_hours = ignore_working_hours
        self.term_builder = SearchTermBuilder()
        self.validation_service = EmailValidationService()
        
        # Recria reposit√≥rios ap√≥s limpeza
        self.json_repo = JsonRepository(VISITED_JSON, SEEN_EMAILS_JSON)
        self.excel_repo = ExcelRepository(OUTPUT_XLSX)
        
        self.visited_domains = self.json_repo.load_visited_domains()
        self.seen_emails = self.json_repo.load_seen_emails()
        self.top_results_total = self._get_processing_mode()
    

    
    def _get_search_engine_option(self) -> str:
        """Obt√©m do usu√°rio qual motor de busca usar"""
        while True:
            try:
                print("\nüîç Escolha o motor de busca:")
                print("1. DuckDuckGo")
                print("2. Google Chrome")
                option = input("Digite sua op√ß√£o (1/2 - padr√£o: 1): ").strip()
                
                if not option or option == '1':
                    return "DUCKDUCKGO"
                elif option == '2':
                    return "GOOGLE"
                else:
                    print("[ERRO] Digite '1' para DuckDuckGo ou '2' para Google Chrome")
            except:
                print("[ERRO] Entrada inv√°lida")
    
    def _get_restart_option(self) -> bool:
        """Obt√©m do usu√°rio se deve reiniciar do zero"""
        while True:
            try:
                option = input("\nüîÑ Reiniciar busca do zero? (s/n - padr√£o: n): ").lower().strip()
                if not option or option == 'n':
                    return False  # Continuar
                elif option == 's':
                    return True   # Reiniciar
                else:
                    print("[ERRO] Digite 's' para reiniciar ou 'n' para continuar")
            except:
                print("[ERRO] Entrada inv√°lida")
    
    def _clear_all_data(self):
        """Limpa todos os arquivos de dados"""
        # Garante que pasta data existe
        os.makedirs(DATA_DIR, exist_ok=True)
        
        files_to_clear = [VISITED_JSON, SEEN_EMAILS_JSON, OUTPUT_XLSX]
        
        for file_path in files_to_clear:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"[INFO] Arquivo {file_path} removido")
            except Exception as e:
                print(f"[AVISO] Erro ao remover {file_path}: {e}")
        
        print("[OK] Dados anteriores limpos. Iniciando do zero...")
    

    
    def _get_processing_mode(self) -> int:
        """Obt√©m do usu√°rio o modo de processamento"""
        while True:
            try:
                mode = input("\nüîç Processamento em lote ou completo? (l/c - padr√£o: c): ").lower().strip()
                if not mode or mode == 'c':
                    return 999999  # Processamento completo
                elif mode == 'l':
                    # Pergunta quantidade para lote
                    while True:
                        try:
                            limit = input("Quantos resultados por termo? (padr√£o: 10): ")
                            if not limit.strip():
                                return 10
                            limit = int(limit)
                            if limit > 0:
                                return limit
                            else:
                                print("[ERRO] Digite um n√∫mero maior que zero")
                        except ValueError:
                            print("[ERRO] Digite um n√∫mero v√°lido")
                else:
                    print("[ERRO] Digite 'l' para lote ou 'c' para completo")
            except:
                print("[ERRO] Entrada inv√°lida")
    
    def _check_and_install_chromedriver(self) -> bool:
        """Verifica e instala ChromeDriver se necess√°rio"""
        if os.path.exists("drivers/chromedriver.exe"):
            return True
        
        print("[INFO] ChromeDriver n√£o encontrado. Baixando automaticamente...")
        
        try:
            # Detecta vers√£o do Chrome
            import winreg
            key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r"Software\Google\Chrome\BLBeacon")
            version, _ = winreg.QueryValueEx(key, "version")
            chrome_version = version.split('.')[0]
        except:
            chrome_version = "119"  # Vers√£o padr√£o
        
        try:
            # Busca vers√£o compat√≠vel
            api_url = "https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json"
            response = requests.get(api_url, timeout=10)
            data = response.json()
            
            download_url = None
            for version_info in reversed(data['versions']):
                if version_info['version'].startswith(chrome_version):
                    for download in version_info['downloads'].get('chromedriver', []):
                        if download['platform'] == 'win64':
                            download_url = download['url']
                            break
                    if download_url:
                        break
            
            if not download_url:
                print("[ERRO] N√£o foi poss√≠vel encontrar vers√£o compat√≠vel do ChromeDriver")
                return False
            
            # Baixa e extrai
            print("[INFO] Baixando ChromeDriver...")
            response = requests.get(download_url, timeout=30)
            
            with open("chromedriver.zip", 'wb') as f:
                f.write(response.content)
            
            with zipfile.ZipFile("chromedriver.zip", 'r') as zip_ref:
                # Garante que pasta drivers existe
                os.makedirs('drivers', exist_ok=True)
                
                for file_info in zip_ref.filelist:
                    if file_info.filename.endswith('chromedriver.exe'):
                        with zip_ref.open(file_info) as source, open('drivers/chromedriver.exe', 'wb') as target:
                            target.write(source.read())
                        break
            
            os.remove("chromedriver.zip")
            print("[OK] ChromeDriver instalado com sucesso")
            return True
            
        except Exception as e:
            print(f"[ERRO] Falha ao baixar ChromeDriver: {e}")
            return False
    
    def execute(self) -> bool:
        """Executa coleta completa de e-mails"""
        try:
            # Verifica e instala ChromeDriver se necess√°rio
            if not self._check_and_install_chromedriver():
                print("[ERRO] ChromeDriver n√£o dispon√≠vel")
                return False
            
            if not self.driver_manager.start_driver():
                print("[ERRO] Falha ao iniciar driver")
                return False
            
            # Define driver no scraper Google se necess√°rio
            if self.search_engine == "GOOGLE":
                self.scraper.driver = self.driver_manager.driver
            
            # Lista √∫nica de termos de busca usando constantes
            search_terms = []
            
            # Verifica se √© modo teste via configura√ß√£o
            if IS_TEST_MODE:
                print("[INFO] Modo TESTE ativado via settings.py")
                # Modo teste - apenas alguns termos
                for base in BASE_BUSCA_TESTES:
                    search_terms.append(f"{base} {CIDADE_BASE} capital")
            else:
                print("[INFO] Modo PRODU√á√ÉO - processamento completo")
                # Modo produ√ß√£o - todos os termos
                # Capital
                for base in BASE_BUSCA:
                    search_terms.append(f"{base} {CIDADE_BASE} capital")

                # Zonas
                for base in BASE_BUSCA:
                    for zona in BASE_ZONAS:
                        search_terms.append(f"{base} {zona} {CIDADE_BASE}")

                # Bairros
                for base in BASE_BUSCA:
                    for bairro in BASE_BAIRROS:
                        search_terms.append(f"{base} {bairro} {CIDADE_BASE}")

                # Interior
                for base in BASE_BUSCA:
                    for cidade in CIDADES_INTERIOR:
                        search_terms.append(f"{base} {cidade} {UF_BASE}")

            # Converte para objetos SearchTerm
            terms = [SearchTerm(query=term, location=UF_BASE, category=CATEGORIA_BASE, pages=10) for term in search_terms]
            
            return self.collect_emails(terms)
            
        finally:
            self.driver_manager.close_driver()
    
    def collect_emails(self, terms: List[SearchTerm]) -> bool:
        """Coleta e-mails usando termos de busca"""
        total_saved = 0
        
        # Calcula total real de resultados esperados
        if self.top_results_total > 1000:  # Modo completo
            total_expected_results = len(terms) * RESULTS_PER_TERM_LIMIT
        else:  # Modo lote
            total_expected_results = len(terms) * self.top_results_total
        
        global_results_processed = 0  # Contador global
        
        for i, term in enumerate(terms, 1):
            # Verifica hor√°rio de trabalho apenas se n√£o foi ignorado pelo usu√°rio
            if not self.ignore_working_hours:
                while not self.working_hours.is_working_time():
                    print(f"[PAUSA] Fora do hor√°rio ({START_HOUR}:00‚Äì{END_HOUR}:00). Recheco em {OUT_OF_HOURS_WAIT_SECONDS}s.")
                    time.sleep(OUT_OF_HOURS_WAIT_SECONDS)
            
            mode_text = "completo" if self.top_results_total > 1000 else f"lote ({self.top_results_total})"
            print(f"\n[TERMO {i}/{len(terms)}] {term.query} | modo: {mode_text}")
            
            if not self.scraper.search(term.query):
                print("  [ERRO] Busca falhou")
                continue
            
            term_saved = 0
            results_processed = 0  # Contador de resultados processados
            
            for page in range(term.pages):
                # Para se j√° processou o limite de resultados
                if results_processed >= self.top_results_total:
                    break
                    
                links = self.scraper.get_result_links(BLACKLIST_HOSTS)
                if not links:
                    break
                
                for link in links:
                    # Para se j√° processou o limite de resultados
                    if results_processed >= self.top_results_total:
                        break
                        
                    results_processed += 1
                    global_results_processed += 1
                    domain = self.validation_service.extract_domain_from_url(link)
                    
                    if self.visited_domains.get(domain):
                        print(f"\n    [PULAR] Site j√° visitado: {domain}")
                        continue
                    
                    # Marca dom√≠nio como visitado ANTES de extrair dados
                    self.visited_domains[domain] = True
                    print(f"\n    [VISITA] Acessando site: {domain} ({global_results_processed}/{total_expected_results} processado)")
                    
                    company = self.scraper.extract_company_data(link, MAX_EMAILS_PER_SITE)
                    company.search_term = term.query  # Adiciona termo de busca
                    
                    # S√≥ processa se h√° e-mails v√°lidos na string
                    if company.emails and company.emails.strip():
                        # Separa e-mails da string para verificar se j√° foram vistos
                        email_list = [e.strip() for e in company.emails.split(';') if e.strip()]
                        new_emails = [e for e in email_list if e not in self.seen_emails]
                        
                        if new_emails:
                            # Reconstr√≥i string apenas com e-mails novos
                            company.emails = ';'.join(new_emails)
                            self.excel_repo.save_company(company)
                            print(f"    [OK] Empresa salva em: {OUTPUT_XLSX}")
                            
                            # Atualiza controle de e-mails
                            for email in new_emails:
                                self.seen_emails.add(email)
                            
                            term_saved += 1
                            total_saved += 1
                        else:
                            print(f"    [-] E-mails j√° coletados: {domain}")
                    else:
                        print(f"    [-] Sem e-mail v√°lido: {domain}")
                    
                    # Salva progresso sempre
                    self.json_repo.save_visited_domains(self.visited_domains)
                    self.json_repo.save_seen_emails(self.seen_emails)
                    
                    time.sleep(random.uniform(*SEARCH_DWELL))
                
                # Vai para pr√≥xima p√°gina se ainda h√° p√°ginas para processar
                if page < term.pages - 1 and results_processed < self.top_results_total:
                    if hasattr(self.scraper, 'go_to_next_page'):
                        if not self.scraper.go_to_next_page():
                            print(f"    [INFO] N√£o h√° mais p√°ginas para o termo '{term.query}'")
                            break
                    else:
                        # Fallback para scrapers sem go_to_next_page
                        try:
                            self.driver_manager.driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
                            time.sleep(random.uniform(1.0, 1.6))
                        except:
                            pass
            
            print(f"  => Termo conclu√≠do. Novas empresas: {term_saved}")
        
        print(f"\nFINALIZADO. Total de empresas novas: {total_saved}")
        print("[INFO] Processamento conclu√≠do. Encerrando PythonSearchApp...")
        return True